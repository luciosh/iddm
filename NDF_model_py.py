# -*- coding: utf-8 -*-
"""Neural_decision_forest - sem fit.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TSiutzG4tvokfYCkRJbnpEyfFXnZuy3k
"""

import urllib.request
url = "https://github.com/srg-lc/iddm/blob/main/data.zip?raw=true"
print ("download start!")
filename, headers = urllib.request.urlretrieve(url, filename="data.zip")
print ("download complete!")
print ("download file location: ", filename)

# Setup
import tensorflow as tf
import numpy as np
import pandas as pd
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras import callbacks
from sklearn import preprocessing
from matplotlib import pyplot as plt
from sklearn import metrics
import math

CSV_HEADER = [
    "FIRST_SWITCHED",          
    "IN_BYTES",
    "IN_PKTS",
    "IPV4_DST_ADDR",
    "L4_DST_PORT",
    "L4_SRC_PORT",
    "LAST_SWITCHED",          
    "PROTOCOL",
    "SRC_TOS",
    "TCP_FLAGS",
    "IP",
    "DURATION",
    "device_model",
    "partition",          
    "SRC_AS",
    "DST_AS",
    "INPUT_SNMP",
    "OUTPUT_SNMP",
    "IPV4_SRC_ADDR",
    "MAC",          
    "category",          
    "type",          
    "date",          
    "inter_arrival_time",                
]
data_url = (
    "data.zip"
)
data = pd.read_csv(data_url, header=None, low_memory=False, names=CSV_HEADER)
data = data.drop(index=[0])
data = data.reset_index(drop=True)
data.head()

# Filtrando colunas não utilizadas
data = data.drop(columns=['IPV4_SRC_ADDR', 'MAC', 'category', 'type', 'date', 'inter_arrival_time', 'SRC_AS', 'DST_AS', 'INPUT_SNMP', 'OUTPUT_SNMP', 'IP', 'DURATION'])

# Separando Treino e predicao
train_data = data[data['partition'] == "training"]
test_data = data[data['partition'] == "test"]
train_data = train_data.drop(columns=['partition'])
test_data = test_data.drop(columns=['partition'])


train_data.info()

CSV_HEADER = [      
    "FIRST_SWITCHED",
    "IN_BYTES",
    "IN_PKTS",
    "IPV4_DST_ADDR",
    "L4_DST_PORT",
    "L4_SRC_PORT",
    "LAST_SWITCHED",
    "PROTOCOL",
    "SRC_TOS",
    "TCP_FLAGS",
    "device_model",
]

train_data.info()

# train_data.head()

# train_data = train_data.drop(columns=['L4_DST_PORT', 'L4_SRC_PORT', 'PROTOCOL', 'SRC_TOS', 'TCP_FLAGS', 'FIRST_SWITCHED', 'IN_BYTES', 'IN_PKTS', 'LAST_SWITCHED'])

train_data.head()

train_data_file = "train_data.csv"
test_data_file = "test_data.csv"

train_data.to_csv(train_data_file, index=False, header= False)
test_data.to_csv(test_data_file, index=False, header= False)

# A list of the numerical feature names.
NUMERIC_FEATURE_NAMES = [
    "IN_BYTES",
    "IN_PKTS",
    "FIRST_SWITCHED",
    "LAST_SWITCHED",
]
# A dictionary of the categorical features and their vocabulary.
CATEGORICAL_FEATURES_WITH_VOCABULARY = {
    "L4_DST_PORT": sorted(list(train_data["L4_DST_PORT"].unique())),
    "L4_SRC_PORT": sorted(list(train_data["L4_SRC_PORT"].unique())),
    "SRC_TOS": sorted(list(train_data["SRC_TOS"].unique())),
    "TCP_FLAGS": sorted(list(train_data["TCP_FLAGS"].unique())),
    "PROTOCOL": sorted(list(train_data["PROTOCOL"].unique())),
    "IPV4_DST_ADDR": sorted(list(train_data["IPV4_DST_ADDR"].unique())),
    # "device_model": sorted(list(train_data["device_model"].unique())),
}
# A list of the columns to ignore from the dataset.
# IGNORE_COLUMN_NAMES = ["device_model", "IN_BYTES", "IN_PKTS","FIRST_SWITCHED","LAST_SWITCHED", "IPV4_DST_ADDR"]
IGNORE_COLUMN_NAMES = []
# A list of the categorical feature names.
CATEGORICAL_FEATURE_NAMES = list(CATEGORICAL_FEATURES_WITH_VOCABULARY.keys())
# A list of all the input features.
FEATURE_NAMES = NUMERIC_FEATURE_NAMES + CATEGORICAL_FEATURE_NAMES
# A list of column default values for each feature.
COLUMN_DEFAULTS = [
    [0.0] if feature_name in NUMERIC_FEATURE_NAMES + IGNORE_COLUMN_NAMES else ["NA"]
    for feature_name in CSV_HEADER
]
# The name of the target feature.
TARGET_FEATURE_NAME = "device_model"
# A list of the labels of the target features.
TARGET_LABELS =  sorted(list(train_data["device_model"].unique()))

data_test = pd.read_csv(train_data_file, header=None, low_memory=False, names=CSV_HEADER)
data_test.head()

# Create tf.data.Dataset objects for training and validation
from tensorflow.keras.layers import StringLookup

target_label_lookup = StringLookup(
    vocabulary=TARGET_LABELS, mask_token=None, num_oov_indices=0
)


def get_dataset_from_csv(csv_file_path, shuffle=False, batch_size=128):
    dataset = tf.data.experimental.make_csv_dataset(
        csv_file_path,
        batch_size=batch_size,
        column_names=CSV_HEADER,
        column_defaults=COLUMN_DEFAULTS,
        label_name=TARGET_FEATURE_NAME,
        num_epochs=1,
        header=False,
        na_value="?",
        shuffle=shuffle,
    ).map(lambda features, target: (features, target_label_lookup(target)))
    return dataset.cache()

# Create model inputs
# cria dtype para features numéricas e categóricas
def create_model_inputs():
    inputs = {}
    for feature_name in FEATURE_NAMES:
        if feature_name in NUMERIC_FEATURE_NAMES:
            inputs[feature_name] = layers.Input(
                name=feature_name, shape=(), dtype=tf.float32
            )
        else:
            inputs[feature_name] = layers.Input(
                name=feature_name, shape=(), dtype=tf.string
            )
    return inputs

# Encode input features
def encode_inputs(inputs):
    encoded_features = []
    for feature_name in inputs:
        if feature_name in CATEGORICAL_FEATURE_NAMES:
            vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]
            # Create a lookup to convert a string values to an integer indices.
            # Since we are not using a mask token, nor expecting any out of vocabulary
            # (oov) token, we set mask_token to None and num_oov_indices to 0.
            lookup = StringLookup(
                vocabulary=vocabulary, mask_token=None, num_oov_indices=1
            )
            # Convert the string input values into integer indices.
            value_index = lookup(inputs[feature_name])
            embedding_dims = int(math.sqrt(lookup.vocabulary_size()))
            # Create an embedding layer with the specified dimensions.
            embedding = layers.Embedding(
                input_dim=lookup.vocabulary_size(), output_dim=embedding_dims
            )
            # Convert the index values to embedding representations.
            encoded_feature = embedding(value_index)
        else:
            # Use the numerical features as-is.
            encoded_feature = inputs[feature_name]
            if inputs[feature_name].shape[-1] is None:
                encoded_feature = tf.expand_dims(encoded_feature, -1)

        encoded_features.append(encoded_feature)

    encoded_features = layers.concatenate(encoded_features)
    return encoded_features

# Deep Neural Decision Tree
class NeuralDecisionTree(keras.Model):
    def __init__(self, depth, num_features, used_features_rate, num_classes):
        super(NeuralDecisionTree, self).__init__()
        self.depth = depth
        self.num_leaves = 2 ** depth
        self.num_classes = num_classes

        # Create a mask for the randomly selected features.
        num_used_features = int(num_features * used_features_rate)
        one_hot = np.eye(num_features)
        sampled_feature_indicies = np.random.choice(
            np.arange(num_features), num_used_features, replace=False
        )
        self.used_features_mask = one_hot[sampled_feature_indicies]

        # Initialize the weights of the classes in leaves.
        self.pi = tf.Variable(
            initial_value=tf.random_normal_initializer()(
                shape=[self.num_leaves, self.num_classes]
            ),
            dtype="float32",
            trainable=True,
        )

        # Initialize the stochastic routing layer.
        self.decision_fn = layers.Dense(
            units=self.num_leaves, activation="sigmoid", name="decision"
        )

    def call(self, features):
        batch_size = tf.shape(features)[0]

        # Apply the feature mask to the input features.
        features = tf.matmul(
            features, self.used_features_mask, transpose_b=True
        )  # [batch_size, num_used_features]
        # Compute the routing probabilities.
        decisions = tf.expand_dims(
            self.decision_fn(features), axis=2
        )  # [batch_size, num_leaves, 1]
        # Concatenate the routing probabilities with their complements.
        decisions = layers.concatenate(
            [decisions, 1 - decisions], axis=2
        )  # [batch_size, num_leaves, 2]

        mu = tf.ones([batch_size, 1, 1])

        begin_idx = 1
        end_idx = 2
        # Traverse the tree in breadth-first order.
        for level in range(self.depth):
            mu = tf.reshape(mu, [batch_size, -1, 1])  # [batch_size, 2 ** level, 1]
            mu = tf.tile(mu, (1, 1, 2))  # [batch_size, 2 ** level, 2]
            level_decisions = decisions[
                :, begin_idx:end_idx, :
            ]  # [batch_size, 2 ** level, 2]
            mu = mu * level_decisions  # [batch_size, 2**level, 2]
            begin_idx = end_idx
            end_idx = begin_idx + 2 ** (level + 1)

        mu = tf.reshape(mu, [batch_size, self.num_leaves])  # [batch_size, num_leaves]
        probabilities = keras.activations.softmax(self.pi)  # [num_leaves, num_classes]
        outputs = tf.matmul(mu, probabilities)  # [batch_size, num_classes]
        return outputs

# Deep Neural Decision Forest
class NeuralDecisionForest(keras.Model):
    def __init__(self, num_trees, depth, num_features, used_features_rate, num_classes):
        super(NeuralDecisionForest, self).__init__()
        self.ensemble = []
        # Initialize the ensemble by adding NeuralDecisionTree instances.
        # Each tree will have its own randomly selected input features to use.
        for _ in range(num_trees):
            self.ensemble.append(
                NeuralDecisionTree(depth, num_features, used_features_rate, num_classes)
            )

    def call(self, inputs):
        # Initialize the outputs: a [batch_size, num_classes] matrix of zeros.
        batch_size = tf.shape(inputs)[0]
        outputs = tf.zeros([batch_size, num_classes])

        # Aggregate the outputs of trees in the ensemble.
        for tree in self.ensemble:
            outputs += tree(inputs)
        # Divide the outputs by the ensemble size to get the average.
        outputs /= len(self.ensemble)
        return outputs

learning_rate = 0.0001
batch_size = 265
num_epochs = 1
hidden_units = [64, 64]


def run_experiment(model, model_type):

    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),
        loss=keras.losses.SparseCategoricalCrossentropy(),
        metrics=[keras.metrics.SparseCategoricalAccuracy()],
    )
    metrica=keras.metrics.SparseCategoricalAccuracy()

    print("Start training the model...")
    train_dataset = get_dataset_from_csv(
        train_data_file, shuffle=True, batch_size=batch_size
    )
    # print(train_dataset)
    earlyStopping = tf.keras.callbacks.EarlyStopping(monitor="sparse_categorical_accuracy", patience=7, verbose=0, mode='max', restore_best_weights = True)
    # mcp_save = tf.keras.callbacks.ModelCheckpoint('.mdl_wts.hdf5', save_best_only=True, monitor="loss", mode='min')
    reduce_lr_loss = tf.keras.callbacks.ReduceLROnPlateau(monitor="sparse_categorical_accuracy", factor=0.2, patience=5, verbose=1, min_delta=0.0001, mode='max')

    history = model.fit(train_dataset, epochs=num_epochs, callbacks=[earlyStopping, reduce_lr_loss])
    # model.fit(train_dataset, epochs=num_epochs)
    print("Model training finished")
    model_name = "model_ndf" + str(model_type)
    model.save(model_name)
    print("Evaluating the model on the test data...")
    model = keras.models.load_model(model_name)
    test_dataset = get_dataset_from_csv(test_data_file, batch_size=batch_size)

    _, accuracy = model.evaluate(test_dataset)
    print("------ Acuracia ---------")

    print(f"Test accuracy: {round(accuracy * 100, 2)}%")
   
    print("------ Grafico de acuracia e Loss ---------")
    # https://www.ti-enxame.com/pt/python/grafico-de-treinamento-keras-tensorflow-em-tempo-real/833518371/
    print(history.history.keys())
    # summarize history for accuracy
    plt.plot(history.history['sparse_categorical_accuracy'])
    plt.plot(history.history['sparse_categorical_accuracy'])
    plt.title('model accuracy')
    plt.ylabel('accuracy')
    plt.xlabel('Epoch')
    plt.legend(['train', 'test'], loc='upper left')
    plt.show()
    # summarize history for loss
    plt.plot(history.history['loss'])
    plt.plot(history.history['loss'])
    plt.title('model loss')
    plt.ylabel('loss')
    plt.xlabel('Epoch')
    plt.legend(['train', 'test'], loc='upper left')
    plt.show()

    print("------ Confusion matrix ---------")
   
    y_pred = model.predict(test_dataset)
    y_teste = model.get_layer(name = 'device_model')
    confusion_matrix = metrics.confusion_matrix(y_teste, np.rint(y_pred))
    print(confusion_matrix)
    # https://stackoverflow.com/questions/56458526/get-confusion-matrix-from-a-keras-model



# Experiment 1: train a decision tree model
num_trees = 3
depth = 3
used_features_rate = 1.0
num_classes = len(TARGET_LABELS)


def create_tree_model():
    inputs = create_model_inputs()
    features = encode_inputs(inputs)
    features = layers.BatchNormalization()(features)
    num_features = features.shape[1]

    tree = NeuralDecisionTree(depth, num_features, used_features_rate, num_classes)

    outputs = tree(features)
    model = keras.Model(inputs=inputs, outputs=outputs)
    return model


tree_model = create_tree_model()
run_experiment(tree_model, 1)

# Experiment 2: train a forest model
num_trees = 25
depth = 5
used_features_rate = 0.5
num_classes = len(TARGET_LABELS)


def create_forest_model():
    inputs = create_model_inputs()
    features = encode_inputs(inputs)
    features = layers.BatchNormalization()(features)
    num_features = features.shape[1]

    forest_model = NeuralDecisionForest(
        num_trees, depth, num_features, used_features_rate, num_classes
    )

    outputs = forest_model(features)
    model = keras.Model(inputs=inputs, outputs=outputs)
    return model



forest_model = create_forest_model()
run_experiment(forest_model, 2)


"""-------------------------------------------------------------------------------"""

# # Relatorio de metricas
# print(classification_report(ypred, Y_test))

# # Metrica usada no artigo e que não encontrei como rodar sem onehotencoder
# from sklearn.metrics import precision_score
# print(average_precision_score(ypred, Y_test))

"""Codigo para converter ip em inteiro
import socket, struct def ip2long(ip): packedIP = socket.inet_aton(ip) return struct.unpack("!L", packedIP)[0]
"""